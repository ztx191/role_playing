# -*- coding: utf-8 -*-
# LLM Config
LLM_NAME_OR_PATH = D:\model\Qwen2.5-0.5B-Instruct
# LLM_NAME_OR_PATH = D:\model\Qwen1.5-1.8B-chat
LLM_QUANTIZED = False
LLM_TEMPERATURE = 0.1
LLM_MAX_NEW_TOKEN = 1024
LLM_DEVICE = cuda
LLM_TORCH_TYPE = float32
LLM_CACHE_DIR =
LLM_LORA_PATH =
LLM_BASE_API = https://api.aiproxy.io/v1
# LLM_BASE_API = http://127.0.0.1:11434/v1
LLM_API_KEY = sk-W6yiaxzMXDVmfT0mOGPxy2PbJfOJivW8kZHgtZ6ghlWWAqY4
LLM_NAME = gpt-3.5-turbo
# LLM_NAME = qwen2.5:0.5b
LLM_STREAM = False


# Role Setting
ROLE_SYSTEM_SETTING = model_template/prompt/system_setting.txt
ROLE_CHARACTER_CARDS_PATH = model_template/character_cards
ROLE_HISTORY_PROMPT = model_template/prompt/history_prompt.txt
ROLE_OPENING_REMARKS = model_template/prompt/opening_remarks.txt
ROLE_RAG_PROMPT = model_template/prompt/rag_prompt.txt
ROLE_CHAT_HISTORY_PATH = chat_history
# size是不为零的偶数
ROLE_CHAT_HISTORY_SIZE = 8

# RAG Setting
RAG_EMBEDDING_MODEL =
RAG_RANKER_MODEL =
RAG_VECTOR_DB =
RAG_TOK_K = 3

